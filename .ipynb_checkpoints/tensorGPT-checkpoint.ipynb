{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d424de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1dd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8c9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = tf.convert_to_tensor(encode(text), dtype=tf.int64)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a85083",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.random.Generator.from_seed(1331)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = g1.uniform(minval=0,maxval=len(data)-block_size,shape=[batch_size,],dtype=tf.dtypes.int64)\n",
    "    \n",
    "    x = tf.map_fn(lambda i:tf.gather(data, tf.range(i,i+block_size)), ix,dtype=tf.dtypes.int64)\n",
    "    y = tf.map_fn(lambda i:tf.gather(data, tf.range(i+1,i+block_size+1)), ix,dtype=tf.dtypes.int64)\n",
    "\n",
    "    return tf.stack(x), tf.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2794b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d other skins\n",
      "Of ill-shaped fishes; and about his shelves\n",
      "A beggarly account of empty boxes,\n",
      "Green earthen pots, bladders and musty seeds,\n",
      "Remnants of packthread and old cakes of roses,\n",
      "Were thinly scatter'd, to make up a show.\n",
      "Noting this penury, to mysel\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('train')\n",
    "print(decode(x[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5bdce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " other skins\n",
      "Of ill-shaped fishes; and about his shelves\n",
      "A beggarly account of empty boxes,\n",
      "Green earthen pots, bladders and musty seeds,\n",
      "Remnants of packthread and old cakes of roses,\n",
      "Were thinly scatter'd, to make up a show.\n",
      "Noting this penury, to myself\n"
     ]
    }
   ],
   "source": [
    "print(decode(y[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0171d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANIO:\n",
      "But say, what to thine old news?\n",
      "\n",
      "BIONDELLO:\n",
      "Why, Petruchio is coming in a new hat and an old\n",
      "jerkin, a pair of old breeches thrice turned, a pair\n",
      "of boots that have been candle-cases, one buckled,\n",
      "another laced, an old rusty sword ta'en out of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('val')\n",
    "print(decode(x[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba096a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIO:\n",
      "But say, what to thine old news?\n",
      "\n",
      "BIONDELLO:\n",
      "Why, Petruchio is coming in a new hat and an old\n",
      "jerkin, a pair of old breeches thrice turned, a pair\n",
      "of boots that have been candle-cases, one buckled,\n",
      "another laced, an old rusty sword ta'en out of the\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "print(decode(y[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa06e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses =[0]*eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.numpy().tolist()\n",
    "        out[split] = tf.math.reduce_mean(tf.constant(losses))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f039c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(layers.Layer):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.query = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.value = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.tril = tf.linalg.LinearOperatorLowerTriangular(tf.ones((block_size, block_size)))\n",
    "        #self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        t = tf.transpose(k, perm=[0, 2, 1])\n",
    "        wei = q @ t * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = tf.where(tf.equal(self.tril.to_dense()[None, :T, :T], 0), float('-inf'), wei) # (B, T, T)\n",
    "        wei = tf.nn.softmax(wei,axis=-1) # (B, T, T)\n",
    "        #wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441ab06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "        self.proj = layers.Dense(n_embd,input_shape=(None,head_size * num_heads))\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        out = tf.concat(head_outputs, axis=-1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c04aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "class FeedFoward(layers.Layer):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedFoward,self).__init__()\n",
    "        self.net = Sequential([layers.Dense(4 * n_embd),\n",
    "                               layers.ReLU(),\n",
    "                               layers.Dense(n_embd)#,\n",
    "                               #layers.Dropout(dropout)\n",
    "                              ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e86068cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super(Block,self).__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x):\n",
    "        s = self.sa(self.ln1(x))\n",
    "        x = x + s\n",
    "        f = self.ffwd(self.ln2(x))\n",
    "        x = x + f\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231b0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GPTLanguageModel, self).__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd)\n",
    "        self.blocks = keras.Sequential([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = layers.LayerNormalization() # final layer norm\n",
    "        self.lm_head = layers.Dense(vocab_size,input_shape=(None,4 * n_embd))\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self._init_weights()\n",
    "        \n",
    "\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        for module in self.submodules:\n",
    "            if isinstance(module, layers.Dense):\n",
    "                module.kernel_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "                if module.use_bias:\n",
    "                    module.bias_initializer = keras.initializers.Zeros()\n",
    "            elif isinstance(module, layers.Embedding):\n",
    "                module.embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int64)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = tf.reshape(logits, (B*T, C))\n",
    "            targets = tf.reshape(targets, B*T)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tf.random.categorical(probs, num_samples=1, dtype=tf.int64)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def saveModel(self):\n",
    "        x = self.weights\n",
    "        folder = \"model/\"\n",
    "        for i in x:\n",
    "            fileName = str(i.name.replace('/','__').replace(':','##')) +\".txt\"\n",
    "            f = open(str(folder+fileName),\"w\")\n",
    "            numpy.savetxt(f,i)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9a6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969770c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37b886bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2230, val loss 4.2257\n",
      "\n",
      "!Cja&Bt!-?x!PEdr\n",
      "JVpos\n",
      ".EaeVjvQUR&yPOHW diXiw3tLm.o;$A&KwMsZJPYyn?K$cDalfBuKv,e$,Gl;Iwr UCOHsfhhVgrz \n",
      "======================\n",
      "\n",
      "runtime loss: 4.222377777099609\n",
      "runtime loss: 4.220323085784912\n",
      "runtime loss: 4.224641799926758\n",
      "runtime loss: 4.225830554962158\n",
      "runtime loss: 4.232521057128906\n",
      "runtime loss: 4.221375465393066\n",
      "runtime loss: 4.230021953582764\n",
      "runtime loss: 4.222139835357666\n",
      "runtime loss: 4.23191499710083\n",
      "runtime loss: 4.219481468200684\n",
      "runtime loss: 4.2262282371521\n",
      "runtime loss: 4.219703197479248\n",
      "runtime loss: 4.222734451293945\n",
      "runtime loss: 4.225307464599609\n",
      "runtime loss: 4.226510524749756\n",
      "runtime loss: 4.221758842468262\n",
      "runtime loss: 4.2211174964904785\n",
      "runtime loss: 4.221519947052002\n",
      "runtime loss: 4.219064235687256\n",
      "runtime loss: 4.213245391845703\n",
      "runtime loss: 4.2183451652526855\n",
      "runtime loss: 4.21750020980835\n",
      "runtime loss: 4.223964214324951\n",
      "runtime loss: 4.221581935882568\n",
      "runtime loss: 4.222181797027588\n",
      "runtime loss: 4.221551418304443\n",
      "runtime loss: 4.228210926055908\n",
      "runtime loss: 4.228638172149658\n",
      "runtime loss: 4.22108268737793\n",
      "runtime loss: 4.219906330108643\n",
      "runtime loss: 4.212671756744385\n",
      "runtime loss: 4.224097728729248\n",
      "runtime loss: 4.223510265350342\n",
      "runtime loss: 4.2230730056762695\n",
      "runtime loss: 4.22289514541626\n",
      "runtime loss: 4.228660583496094\n",
      "runtime loss: 4.212167263031006\n",
      "runtime loss: 4.2186055183410645\n",
      "runtime loss: 4.218559741973877\n",
      "runtime loss: 4.2206950187683105\n",
      "runtime loss: 4.231166362762451\n",
      "runtime loss: 4.219753265380859\n",
      "runtime loss: 4.217932224273682\n",
      "runtime loss: 4.225342273712158\n",
      "runtime loss: 4.225294589996338\n",
      "runtime loss: 4.215953826904297\n",
      "runtime loss: 4.220783233642578\n",
      "runtime loss: 4.225055694580078\n",
      "runtime loss: 4.220149040222168\n",
      "runtime loss: 4.226230621337891\n",
      "runtime loss: 4.222177982330322\n",
      "runtime loss: 4.218690872192383\n",
      "runtime loss: 4.2178449630737305\n",
      "runtime loss: 4.224252700805664\n",
      "runtime loss: 4.221176624298096\n",
      "runtime loss: 4.217810153961182\n",
      "runtime loss: 4.223753452301025\n",
      "runtime loss: 4.213882923126221\n",
      "runtime loss: 4.229370594024658\n",
      "runtime loss: 4.226785182952881\n",
      "runtime loss: 4.218860149383545\n",
      "runtime loss: 4.225447177886963\n",
      "runtime loss: 4.228950023651123\n",
      "runtime loss: 4.221786022186279\n",
      "runtime loss: 4.2240376472473145\n",
      "runtime loss: 4.222247123718262\n",
      "runtime loss: 4.218244552612305\n",
      "runtime loss: 4.226897239685059\n",
      "runtime loss: 4.224192142486572\n",
      "runtime loss: 4.219491481781006\n",
      "runtime loss: 4.224658489227295\n",
      "runtime loss: 4.223147392272949\n",
      "runtime loss: 4.227841854095459\n",
      "runtime loss: 4.2242207527160645\n",
      "runtime loss: 4.217670917510986\n",
      "runtime loss: 4.230128765106201\n",
      "runtime loss: 4.224620342254639\n",
      "runtime loss: 4.225954532623291\n",
      "runtime loss: 4.221150875091553\n",
      "runtime loss: 4.2216386795043945\n",
      "runtime loss: 4.231108665466309\n",
      "runtime loss: 4.22772216796875\n",
      "runtime loss: 4.222959995269775\n",
      "runtime loss: 4.225663661956787\n",
      "runtime loss: 4.218757152557373\n",
      "runtime loss: 4.219714641571045\n",
      "runtime loss: 4.234739780426025\n",
      "runtime loss: 4.225950717926025\n",
      "runtime loss: 4.229406356811523\n",
      "runtime loss: 4.2272796630859375\n",
      "runtime loss: 4.23042631149292\n",
      "runtime loss: 4.219945430755615\n",
      "runtime loss: 4.218080043792725\n",
      "runtime loss: 4.227285385131836\n",
      "runtime loss: 4.213887691497803\n",
      "runtime loss: 4.223519802093506\n",
      "runtime loss: 4.224130153656006\n",
      "runtime loss: 4.222262859344482\n",
      "runtime loss: 4.21063756942749\n",
      "runtime loss: 4.224701404571533\n",
      "runtime loss: 4.227879047393799\n",
      "runtime loss: 4.222854137420654\n",
      "runtime loss: 4.226689338684082\n",
      "runtime loss: 4.228732585906982\n",
      "runtime loss: 4.229417324066162\n",
      "runtime loss: 4.223463535308838\n",
      "runtime loss: 4.219620227813721\n",
      "runtime loss: 4.218986988067627\n",
      "runtime loss: 4.215037822723389\n",
      "runtime loss: 4.219730854034424\n",
      "runtime loss: 4.228647708892822\n",
      "runtime loss: 4.230849742889404\n",
      "runtime loss: 4.221015930175781\n",
      "runtime loss: 4.21414852142334\n",
      "runtime loss: 4.2208051681518555\n",
      "runtime loss: 4.224639415740967\n",
      "runtime loss: 4.221652984619141\n",
      "runtime loss: 4.234377384185791\n",
      "runtime loss: 4.2289719581604\n",
      "runtime loss: 4.221330165863037\n",
      "runtime loss: 4.227172374725342\n",
      "runtime loss: 4.227650165557861\n",
      "runtime loss: 4.216672420501709\n",
      "runtime loss: 4.229891300201416\n",
      "runtime loss: 4.220078945159912\n",
      "runtime loss: 4.220522880554199\n",
      "runtime loss: 4.224058151245117\n",
      "runtime loss: 4.2150654792785645\n",
      "runtime loss: 4.230785846710205\n",
      "runtime loss: 4.218159198760986\n",
      "runtime loss: 4.214125156402588\n",
      "runtime loss: 4.223385334014893\n",
      "runtime loss: 4.227453231811523\n",
      "runtime loss: 4.2292914390563965\n",
      "runtime loss: 4.217541217803955\n",
      "runtime loss: 4.2173027992248535\n",
      "runtime loss: 4.219770431518555\n",
      "runtime loss: 4.224484443664551\n",
      "runtime loss: 4.229307174682617\n",
      "runtime loss: 4.222204685211182\n",
      "runtime loss: 4.223936557769775\n",
      "runtime loss: 4.230453014373779\n",
      "runtime loss: 4.225156307220459\n",
      "runtime loss: 4.2214035987854\n",
      "runtime loss: 4.221366882324219\n",
      "runtime loss: 4.232273101806641\n",
      "runtime loss: 4.223597049713135\n",
      "runtime loss: 4.226742267608643\n",
      "runtime loss: 4.2199788093566895\n",
      "runtime loss: 4.220460891723633\n",
      "runtime loss: 4.2149786949157715\n",
      "runtime loss: 4.219156742095947\n",
      "runtime loss: 4.217458724975586\n",
      "runtime loss: 4.229067325592041\n",
      "runtime loss: 4.216302394866943\n",
      "runtime loss: 4.214505195617676\n",
      "runtime loss: 4.221950531005859\n",
      "runtime loss: 4.2208476066589355\n",
      "runtime loss: 4.229424476623535\n",
      "runtime loss: 4.220950126647949\n",
      "runtime loss: 4.219424724578857\n",
      "runtime loss: 4.225647449493408\n",
      "runtime loss: 4.220022678375244\n",
      "runtime loss: 4.2228899002075195\n",
      "runtime loss: 4.225843906402588\n",
      "runtime loss: 4.215940475463867\n",
      "runtime loss: 4.226762771606445\n",
      "runtime loss: 4.226754665374756\n",
      "runtime loss: 4.220588684082031\n",
      "runtime loss: 4.224095821380615\n",
      "runtime loss: 4.214987754821777\n",
      "runtime loss: 4.2175774574279785\n",
      "runtime loss: 4.226245880126953\n",
      "runtime loss: 4.22652530670166\n",
      "runtime loss: 4.218705654144287\n",
      "runtime loss: 4.229024410247803\n",
      "runtime loss: 4.223025321960449\n",
      "runtime loss: 4.2262864112854\n",
      "runtime loss: 4.220613479614258\n",
      "runtime loss: 4.21702241897583\n",
      "runtime loss: 4.216126918792725\n",
      "runtime loss: 4.208898544311523\n",
      "runtime loss: 4.219067573547363\n",
      "runtime loss: 4.22126579284668\n",
      "runtime loss: 4.217866897583008\n",
      "runtime loss: 4.224541664123535\n",
      "runtime loss: 4.223880767822266\n",
      "runtime loss: 4.226848602294922\n",
      "runtime loss: 4.2262492179870605\n",
      "runtime loss: 4.220840930938721\n",
      "runtime loss: 4.225427627563477\n",
      "runtime loss: 4.217911243438721\n",
      "runtime loss: 4.217904090881348\n",
      "runtime loss: 4.223080158233643\n",
      "runtime loss: 4.216049671173096\n",
      "runtime loss: 4.220576763153076\n",
      "runtime loss: 4.224050521850586\n",
      "runtime loss: 4.22722053527832\n",
      "runtime loss: 4.220579624176025\n",
      "runtime loss: 4.219663619995117\n",
      "runtime loss: 4.216230392456055\n",
      "runtime loss: 4.2216267585754395\n",
      "runtime loss: 4.216026782989502\n",
      "runtime loss: 4.231581211090088\n",
      "runtime loss: 4.2287068367004395\n",
      "runtime loss: 4.218627452850342\n",
      "runtime loss: 4.220550537109375\n",
      "runtime loss: 4.222629070281982\n",
      "runtime loss: 4.212636947631836\n",
      "runtime loss: 4.219510078430176\n",
      "runtime loss: 4.225969314575195\n",
      "runtime loss: 4.226569652557373\n",
      "runtime loss: 4.221549987792969\n",
      "runtime loss: 4.221463680267334\n",
      "runtime loss: 4.217121601104736\n",
      "runtime loss: 4.230082988739014\n",
      "runtime loss: 4.222845554351807\n",
      "runtime loss: 4.226182460784912\n",
      "runtime loss: 4.223443031311035\n",
      "runtime loss: 4.2183403968811035\n",
      "runtime loss: 4.2272210121154785\n",
      "runtime loss: 4.219010353088379\n",
      "runtime loss: 4.2254719734191895\n",
      "runtime loss: 4.221689701080322\n",
      "runtime loss: 4.224067687988281\n",
      "runtime loss: 4.216888427734375\n",
      "runtime loss: 4.2283034324646\n",
      "runtime loss: 4.221150875091553\n",
      "runtime loss: 4.220649719238281\n",
      "runtime loss: 4.218081951141357\n",
      "runtime loss: 4.226236820220947\n",
      "runtime loss: 4.216224670410156\n",
      "runtime loss: 4.222054958343506\n",
      "runtime loss: 4.223734378814697\n",
      "runtime loss: 4.217283248901367\n",
      "runtime loss: 4.2193989753723145\n",
      "runtime loss: 4.232515811920166\n",
      "runtime loss: 4.224747180938721\n",
      "runtime loss: 4.221654891967773\n",
      "runtime loss: 4.218885898590088\n",
      "runtime loss: 4.21908712387085\n",
      "runtime loss: 4.231652736663818\n",
      "runtime loss: 4.2242584228515625\n",
      "runtime loss: 4.230151653289795\n",
      "runtime loss: 4.214758396148682\n",
      "runtime loss: 4.223186492919922\n",
      "runtime loss: 4.21799373626709\n",
      "runtime loss: 4.2225022315979\n",
      "runtime loss: 4.22387170791626\n",
      "runtime loss: 4.217076778411865\n",
      "runtime loss: 4.229766845703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime loss: 4.21252965927124\n",
      "runtime loss: 4.2278547286987305\n",
      "runtime loss: 4.222595691680908\n",
      "runtime loss: 4.227462291717529\n",
      "runtime loss: 4.222540855407715\n",
      "runtime loss: 4.223679065704346\n",
      "runtime loss: 4.219730377197266\n",
      "runtime loss: 4.220093727111816\n",
      "runtime loss: 4.22197151184082\n",
      "runtime loss: 4.221421718597412\n",
      "runtime loss: 4.228562355041504\n",
      "runtime loss: 4.216951847076416\n",
      "runtime loss: 4.227801322937012\n",
      "runtime loss: 4.213322639465332\n",
      "runtime loss: 4.232898235321045\n",
      "runtime loss: 4.226783752441406\n",
      "runtime loss: 4.213698863983154\n",
      "runtime loss: 4.2241058349609375\n",
      "runtime loss: 4.212131977081299\n",
      "runtime loss: 4.222888946533203\n",
      "runtime loss: 4.229415416717529\n",
      "runtime loss: 4.225991725921631\n",
      "runtime loss: 4.229435443878174\n",
      "runtime loss: 4.225467205047607\n",
      "runtime loss: 4.2181172370910645\n",
      "runtime loss: 4.217928409576416\n",
      "runtime loss: 4.2266364097595215\n",
      "runtime loss: 4.231353282928467\n",
      "runtime loss: 4.227865695953369\n",
      "runtime loss: 4.22265625\n",
      "runtime loss: 4.228209972381592\n",
      "runtime loss: 4.220835208892822\n",
      "runtime loss: 4.217565059661865\n",
      "runtime loss: 4.216681003570557\n",
      "runtime loss: 4.227908611297607\n",
      "runtime loss: 4.223930835723877\n",
      "runtime loss: 4.226685047149658\n",
      "runtime loss: 4.231448650360107\n",
      "runtime loss: 4.220154285430908\n",
      "runtime loss: 4.224575519561768\n",
      "runtime loss: 4.218050479888916\n",
      "runtime loss: 4.2281036376953125\n",
      "runtime loss: 4.214244842529297\n",
      "runtime loss: 4.230254650115967\n",
      "runtime loss: 4.227760314941406\n",
      "runtime loss: 4.216601848602295\n",
      "runtime loss: 4.219330310821533\n",
      "runtime loss: 4.22562313079834\n",
      "runtime loss: 4.221327304840088\n",
      "step 300: train loss 4.2229, val loss 4.2245\n",
      "\n",
      "qyYsfQw$p F.qGHD'\n",
      "XcPVYzTmRafvlE?uS&?;dL\n",
      "3XlI-JixhuvcABoa&dO.CdcoIN;hfZflgaqy?RqI kSBAeBWpc3$rApF?Tp \n",
      "======================\n",
      "\n",
      "runtime loss: 4.226111888885498\n",
      "runtime loss: 4.2215986251831055\n",
      "runtime loss: 4.22116756439209\n",
      "runtime loss: 4.222597599029541\n",
      "runtime loss: 4.220909118652344\n",
      "runtime loss: 4.226330280303955\n",
      "runtime loss: 4.215684413909912\n",
      "runtime loss: 4.2221903800964355\n",
      "runtime loss: 4.226757526397705\n",
      "runtime loss: 4.225911617279053\n",
      "runtime loss: 4.212751388549805\n",
      "runtime loss: 4.222378730773926\n",
      "runtime loss: 4.225242614746094\n",
      "runtime loss: 4.221964359283447\n",
      "runtime loss: 4.231191635131836\n",
      "runtime loss: 4.215376377105713\n",
      "runtime loss: 4.228759288787842\n",
      "runtime loss: 4.2229180335998535\n",
      "runtime loss: 4.23220157623291\n",
      "runtime loss: 4.226085186004639\n",
      "runtime loss: 4.216049671173096\n",
      "runtime loss: 4.222854137420654\n",
      "runtime loss: 4.229312419891357\n",
      "runtime loss: 4.214298725128174\n",
      "runtime loss: 4.229768753051758\n",
      "runtime loss: 4.230212211608887\n",
      "runtime loss: 4.221654891967773\n",
      "runtime loss: 4.221063137054443\n",
      "runtime loss: 4.215925693511963\n",
      "runtime loss: 4.226358413696289\n",
      "runtime loss: 4.214326858520508\n",
      "runtime loss: 4.214507579803467\n",
      "runtime loss: 4.2149786949157715\n",
      "runtime loss: 4.223365783691406\n",
      "runtime loss: 4.22092866897583\n",
      "runtime loss: 4.217663288116455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_weights))\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msaveModel()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:301\u001b[0m, in \u001b[0;36m_SoftmaxGrad\u001b[1;34m(op, grad_softmax)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The derivative of the softmax nonlinearity.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03mWe assume that probs is of shape [batch_size * dim]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m \n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m softmax \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 301\u001b[0m sum_channels \u001b[38;5;241m=\u001b[39m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_softmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (grad_softmax \u001b[38;5;241m-\u001b[39m sum_channels) \u001b[38;5;241m*\u001b[39m softmax\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2312\u001b[0m, in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.reduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   2250\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum\u001b[39m(input_tensor, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \n\u001b[0;32m   2254\u001b[0m \u001b[38;5;124;03m  This is the reduction operation for the elementwise `tf.math.add` op.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2312\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce_sum_with_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m_ReductionDims\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2324\u001b[0m, in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum_with_dims\u001b[39m(input_tensor,\n\u001b[0;32m   2317\u001b[0m                          axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2318\u001b[0m                          keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2319\u001b[0m                          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2320\u001b[0m                          dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2321\u001b[0m   keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[0;32m   2322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[0;32m   2323\u001b[0m       keepdims, axis,\n\u001b[1;32m-> 2324\u001b[0m       \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:11227\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  11225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  11226\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 11227\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11228\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeep_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  11230\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = tf.optimizers.experimental.AdamW(learning_rate=learning_rate)\n",
    "context = tf.zeros((1,1), dtype=tf.int64)\n",
    "\n",
    "for i in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(decode(model.generate(context, max_new_tokens=100)[0].numpy().tolist()),\"\\n======================\\n\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb, yb)\n",
    "        print(f\"runtime loss: {loss}\")\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "model.saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a5155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24265085",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddac25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157c947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecee95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd2c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#import numpy\n",
    "#x = model.weights\n",
    "#folder = \"model/\"\n",
    "#for i in x:\n",
    "#    fileName = str(i.name.replace('/','__').replace(':','##')) +\".txt\"\n",
    "#    f = open(str(folder+fileName),\"w\")\n",
    "#    numpy.savetxt(f,i)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59639d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = tf.zeros((1,1), dtype=tf.int64)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 0: train loss 4.1344, val11 loss 4.1358\n",
    "#step 500: train loss 1.9654, val1 loss 2.0608\n",
    "#step 1000: train loss 1.5308, val1 loss 1.7140\n",
    "#step 1500: train loss 1.3840, val1 loss 1.5959\n",
    "#step 2000: train loss 1.3039, val1 loss 1.5739\n",
    "#step 2500: train loss 1.2459, val1 loss 1.5446\n",
    "#step 3000: train loss 1.1882, val1 loss 1.5414\n",
    "#step 3500: train loss 1.1322, val1 loss 1.5643\n",
    "#step 4000: train loss 1.0745, val1 loss 1.5777\n",
    "#step 4500: train loss 1.0114, val1 loss 1.6254\n",
    "#step 5000: train loss 0.9423, val1 loss 1.6745"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
