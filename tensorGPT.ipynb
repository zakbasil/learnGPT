{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d424de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 50\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1dd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8c9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = tf.convert_to_tensor(encode(text), dtype=tf.int64)\n",
    "n = int(0.95*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a85083",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.random.Generator.from_seed(1331)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = g1.uniform(minval=0,maxval=len(data)-block_size,shape=[batch_size,],dtype=tf.dtypes.int64)\n",
    "    \n",
    "    x = tf.map_fn(lambda i:tf.gather(data, tf.range(i,i+block_size)), ix,dtype=tf.dtypes.int64)\n",
    "    y = tf.map_fn(lambda i:tf.gather(data, tf.range(i+1,i+block_size+1)), ix,dtype=tf.dtypes.int64)\n",
    "\n",
    "    return tf.stack(x), tf.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2794b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d other skins\n",
      "Of ill-shaped fishes; and about his shelves\n",
      "A beggarly account of empty boxes,\n",
      "Green earthen pots, bladders and musty seeds,\n",
      "Remnants of packthread and old cakes of roses,\n",
      "Were thinly scatter'd, to make up a show.\n",
      "Noting this penury, to mysel\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('train')\n",
    "print(decode(x[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5bdce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " other skins\n",
      "Of ill-shaped fishes; and about his shelves\n",
      "A beggarly account of empty boxes,\n",
      "Green earthen pots, bladders and musty seeds,\n",
      "Remnants of packthread and old cakes of roses,\n",
      "Were thinly scatter'd, to make up a show.\n",
      "Noting this penury, to myself\n"
     ]
    }
   ],
   "source": [
    "print(decode(y[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0171d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANIO:\n",
      "But say, what to thine old news?\n",
      "\n",
      "BIONDELLO:\n",
      "Why, Petruchio is coming in a new hat and an old\n",
      "jerkin, a pair of old breeches thrice turned, a pair\n",
      "of boots that have been candle-cases, one buckled,\n",
      "another laced, an old rusty sword ta'en out of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('val')\n",
    "print(decode(x[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba096a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIO:\n",
      "But say, what to thine old news?\n",
      "\n",
      "BIONDELLO:\n",
      "Why, Petruchio is coming in a new hat and an old\n",
      "jerkin, a pair of old breeches thrice turned, a pair\n",
      "of boots that have been candle-cases, one buckled,\n",
      "another laced, an old rusty sword ta'en out of the\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "print(decode(y[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa06e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses =[0]*eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.numpy().tolist()\n",
    "        out[split] = tf.math.reduce_mean(tf.constant(losses))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f039c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(layers.Layer):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.query = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.value = layers.Dense(head_size, use_bias=False,input_shape=(None,n_embd))\n",
    "        self.tril = tf.linalg.LinearOperatorLowerTriangular(tf.ones((block_size, block_size)))\n",
    "        #self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        t = tf.transpose(k, perm=[0, 2, 1])\n",
    "        wei = q @ t * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = tf.where(tf.equal(self.tril.to_dense()[None, :T, :T], 0), float('-inf'), wei) # (B, T, T)\n",
    "        wei = tf.nn.softmax(wei,axis=-1) # (B, T, T)\n",
    "        #wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441ab06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "        self.proj = layers.Dense(n_embd,input_shape=(None,head_size * num_heads))\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        out = tf.concat(head_outputs, axis=-1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c04aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "class FeedFoward(layers.Layer):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedFoward,self).__init__()\n",
    "        self.net = Sequential([layers.Dense(4 * n_embd),\n",
    "                               layers.ReLU(),\n",
    "                               layers.Dense(n_embd)#,\n",
    "                               #layers.Dropout(dropout)\n",
    "                              ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e86068cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super(Block,self).__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x):\n",
    "        s = self.sa(self.ln1(x))\n",
    "        x = x + s\n",
    "        f = self.ffwd(self.ln2(x))\n",
    "        x = x + f\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231b0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class GPTLanguageModel(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GPTLanguageModel, self).__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd)\n",
    "        self.blocks = keras.Sequential([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = layers.LayerNormalization() # final layer norm\n",
    "        self.lm_head = layers.Dense(vocab_size,input_shape=(None,4 * n_embd))\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self._init_weights()\n",
    "        \n",
    "\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        for module in self.submodules:\n",
    "            if isinstance(module, layers.Dense):\n",
    "                module.kernel_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "                if module.use_bias:\n",
    "                    module.bias_initializer = keras.initializers.Zeros()\n",
    "            elif isinstance(module, layers.Embedding):\n",
    "                module.embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int64)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = tf.reshape(logits, (B*T, C))\n",
    "            targets = tf.reshape(targets, B*T)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tf.random.categorical(probs, num_samples=1, dtype=tf.int64)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    def saveModel(self):\n",
    "        folder = \"model_\" + str(time.time()).split('.')[0]\n",
    "        os.mkdir(folder)\n",
    "        x = self.weights\n",
    "        for i in x:\n",
    "            fileName = str(i.name.replace('/','__').replace(':','##')) +\".txt\"\n",
    "            f = open(str(folder+fileName),\"w\")\n",
    "            numpy.savetxt(f,i)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9a6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969770c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b886bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.5750, val loss 2.5763\n",
      "\n",
      "dam3$nnoiPqrgi;'lYSxCBqnNR!EJ$KFxhkqZc3ga$WxXBMYAn.Nq,OPMiSmmEoE;g;?JO lqhE$!PVcIvoikamEAdMRoaXfJriO \n",
      "======================\n",
      "\n",
      "step 100: train loss 2.4946, val loss 2.5031\n",
      "\n",
      "3Sq;G&TQYBNnJUMgqNyXez&aFI$.KNkyrb!fqYYzpks3HunVAVCfsE oWhwEoop.NXYIvafRaybYGuUC?NcJ xaRr\n",
      "qXo?dJRMMu \n",
      "======================\n",
      "\n",
      "step 200: train loss 2.4572, val loss 2.4941\n",
      "\n",
      "c SFuYMqtotnLxc$Sl?Gt\n",
      "Ei,pUoHWOh.pboGyoaxds&UPg';WNkFkZXTeBl3PA3dNf3JNbhfmKkgqdAGKUfl!nL'Y3?&ZhLFd:\n",
      " \n",
      "======================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[0;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m         plotValLoss\u001b[39m.\u001b[39mappend(losses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m      6\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 7\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[0;32m      8\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      9\u001b[0m out[split] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mconstant(losses))\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m, in \u001b[0;36mGPTLanguageModel.call\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     34\u001b[0m pos_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table(tf\u001b[39m.\u001b[39mrange(T, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)) \u001b[39m# (T,C)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m x \u001b[39m=\u001b[39m tok_emb \u001b[39m+\u001b[39m pos_emb \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x) \u001b[39m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\sequential.py:410\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m    412\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    414\u001b[0m     \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     \u001b[39m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[39m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[39m# the next layer.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36mBlock.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[0;32m     15\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m s\n\u001b[0;32m     16\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mMultiHeadAttention.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     head_outputs \u001b[39m=\u001b[39m [head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat(head_outputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     head_outputs \u001b[39m=\u001b[39m [head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads]\n\u001b[0;32m     12\u001b[0m     out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat(head_outputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\basil\\anaconda3\\envs\\learnGPT\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mHead.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m t \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(k, perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m     20\u001b[0m wei \u001b[39m=\u001b[39m q \u001b[39m@\u001b[39m t \u001b[39m*\u001b[39m k\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m# (B, T, hs) @ (B, hs, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m wei \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mwhere(tf\u001b[39m.\u001b[39mequal(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtril\u001b[39m.\u001b[39;49mto_dense()[\u001b[39mNone\u001b[39;00m, :T, :T], \u001b[39m0\u001b[39m), \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m), wei) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m wei \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax(wei,axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#wei = self.dropout(wei)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# perform the weighted aggregation of the values\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator.py:1026\u001b[0m, in \u001b[0;36mLinearOperator.to_dense\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a dense (batch) matrix representing this operator.\"\"\"\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope(name):  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_dense()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_lower_triangular.py:214\u001b[0m, in \u001b[0;36mLinearOperatorLowerTriangular._to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_dense\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 214\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_tril()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_lower_triangular.py:180\u001b[0m, in \u001b[0;36mLinearOperatorLowerTriangular._get_tril\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tril\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Gets the `tril` kwarg, with upper part zero-d out.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m   \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49mmatrix_band_part(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tril, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4804\u001b[0m, in \u001b[0;36mmatrix_band_part\u001b[1;34m(input, num_lower, num_upper, name)\u001b[0m\n\u001b[0;32m   4802\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   4803\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 4804\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   4805\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatrixBandPart\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, num_lower, num_upper)\n\u001b[0;32m   4806\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   4807\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = tf.optimizers.experimental.AdamW(learning_rate=learning_rate)\n",
    "context = tf.zeros((1,1), dtype=tf.int64)\n",
    "plotLoss = []\n",
    "plotValLoss = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        plotValLoss.append(losses['val'])\n",
    "        sample = decode(model.generate(context, max_new_tokens=100)[0].numpy().tolist())\n",
    "        print(sample,\"\\n======================\\n\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb, yb)\n",
    "        plotLoss.append(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "model.saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a5155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24265085",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddac25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([*range(0,max_iters+1,eval_interval)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y1 = np.array(plotLoss)\n",
    "y2 = np.array(plotValLoss)\n",
    "\n",
    "plt.plot([*range(0,max_iters)],y1)\n",
    "plt.plot([*range(0,max_iters+1,eval_interval)],y2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd2c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#import numpy\n",
    "#x = model.weights\n",
    "#folder = \"model/\"\n",
    "#for i in x:\n",
    "#    fileName = str(i.name.replace('/','__').replace(':','##')) +\".txt\"\n",
    "#    f = open(str(folder+fileName),\"w\")\n",
    "#    numpy.savetxt(f,i)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59639d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = tf.zeros((1,1), dtype=tf.int64)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 0: train loss 4.1344, val11 loss 4.1358\n",
    "#step 500: train loss 1.9654, val1 loss 2.0608\n",
    "#step 1000: train loss 1.5308, val1 loss 1.7140\n",
    "#step 1500: train loss 1.3840, val1 loss 1.5959\n",
    "#step 2000: train loss 1.3039, val1 loss 1.5739\n",
    "#step 2500: train loss 1.2459, val1 loss 1.5446\n",
    "#step 3000: train loss 1.1882, val1 loss 1.5414\n",
    "#step 3500: train loss 1.1322, val1 loss 1.5643\n",
    "#step 4000: train loss 1.0745, val1 loss 1.5777\n",
    "#step 4500: train loss 1.0114, val1 loss 1.6254\n",
    "#step 5000: train loss 0.9423, val1 loss 1.6745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c204cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
